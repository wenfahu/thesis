% !TeX root = ../main.tex

% 中英文摘要和关键字

\begin{abstract}
  边缘设备对于智能计算的需求在随着边缘设备计算能力的提升及其因数据的
  丰富越来越复杂的 应用需求而不断提升，结合物联网（IOT）与人工智能（AI）
  的应用 AIoT 也即将对很多领域实现重定义。而边缘设备本身有限的计算能力
  和当前机器学习模型，特别是深度神经网络在边缘设别上针对性的计算优化
  依然是实现边缘智能的一大难点。 
  对于视觉任务而言，卷积神经网络是当前最为主要的模型，而其中的计算瓶颈
  便是卷积操作，加速卷积的实现，是加速卷积网络模型应用在边缘设备上执行
  效率的最为有效也最为根本的手段。

  Winograd 卷积是目前在卷积实现的复杂度而言最优的卷积算法，受制于其大尺度
  卷积计算场景下数值不稳定的缺陷，一般只应用于小尺度的卷积中，即卷积核3x3，
  单个卷积操作输出为2x2 或 4x4 的场景。而近年来卷积神经网络的设计趋势
  正是使用小卷积核的卷积操作。同时可以应用在移动设备的神经网络模型往往
  参数规模会相对较低，在经过量化处理后，参与计算的参数甚至可以满足于移动
  设备芯片的缓存中，可以有效提高在边缘设备的计算吞吐。

  本文针对目前最为主流的移动端CPU 计算架构 ARMv8-A 平台，通过结合神经
  网络量化推理，Winograd 快速卷积算法，以及二者对于移动端硬件条件的
  针对性优化，实现卷积操作和卷积神经网络在实际硬件上的效率提升。


  % 关键词用“英文逗号”分隔
  \thusetup{
    keywords = {Winograd, ARM NEON, 卷积, 量化, GEMM},
  }
\end{abstract}

\begin{abstract*}

  The demand for intelligent computing of edge devices is increasing 
  with the improvement of computation capacity of edge devices and 
  the demand for more and more complex applications due to the 
  richness of data. The application of AIoT combining IOT and 
  artificial intelligence (AI) is also redefining many areas. 
  However, the limited computing power of the edge device itself 
  and the current machine learning model, especially the computation 
  optimization of the deep neural network on the edge of the targeted
  devices is still a major difficulty in achieving edge intelligence.

  For visual tasks, the convolutional neural network is currently the 
  most important model, and the computational bottleneck is the 
  convolution operation. Accelerating the implementation of 
  convolution is the most effective in accelerating the execution 
  efficiency of the convolutional network model on edge devices.
  It is also the most fundamental means.

  Winograd convolution is currently the best convolution algorithm 
  in terms of the complexity of convolution implementation. It is 
  subject to the defect of numerical instability in its large-scale 
  convolution calculation scenario, and is generally only used in 
  small-scale convolutions In recent years, the design trend of 
  convolutional neural networks is the convolution operation using 
  small convolution kernels. At the same time, the neural network 
  model that can be applied to mobile devices often has a relatively
  small parameter size. After quantization, the parameters involved 
  in the calculation can even be satisfied in the cache of the 
  mobile device chip, which can effectively improve the computing 
  throughput of the edge device.

  This article aims at the most mainstream mobile CPU computing 
  architecture ARMv8-A platform, through the combination of neural
  network quantitative reasoning, Winograd fast convolution algorithm, 
  and optimization for the targeted hardware devices, to achieve 
  improved efficiency of convolution operations on real hardware.
  \thusetup{
    keywords* = {Winograd, ARM NEON, Convolution, Quantization, GEMM},
  }
\end{abstract*}
