% !TeX root = ../main.tex

\chapter{引言}
\label{cha:intro}

\section{研究背景}

神经网络赋能嵌入式设备智能是当前智能应用的一大热点。作为神经网络与边缘计算的交叉领域，
这一研究需要调和深度神经网络模型计算资源消耗巨大与嵌入式设备端计算资源有限，功耗低的矛盾，
权衡模型的精度和在嵌入式硬件环境中的运行效率，实现智能应用的高效部署。一方面，在网络模型
设计中，很多适用于低计算复杂度的轻量级模型相继提出，如Mobile Net系列，MnasNet 等，实现
了在低参数量和低计算复杂度的场景在对应任务场景下的高精度表现。另外一方面，关于神经网络的
压缩方法的研究如各类剪枝、量化方法也取得了较为成熟的应用，使得深度网络模型的计算得以简化。


智能手机作为使用最为广泛和频繁的嵌入式设备也便成为边缘智能最为热门的应用场景。同时，智能
化的服务也会为智能手机的使用带来颠覆性的体验。目前主流的智能手机的硬件环境均使用ARMv8指
令架构的芯片，而这一系列的芯片具有SIMD扩展NEON，这一功能使得在ARM平台上实现高效的矩阵计
算成为可能。而矩阵计算正是神经网络模型在计算中的核心。“GEMM is at the heart of deep
 learning”（Pete Warden，technical lead of the mobile and embedded TensorFlow 
 group, in Google's Brain team）。

\subsection{边缘智能计算的必要性}

尽管云计算的高速发展和越来越快的网络技术（5G网络）正在深刻的改变着数据获取以及被处理的
手段，很多计算和业务也有着向云端靠拢的趋势，但是在可见的未来，在边缘设备上智能计算的需求
明显是远远高于在云端的。尽管云端的智能计算比如Microsoft Azure Conginitive Serivce，
AWS SageMaker等服务具有丰富的计算资源和应用生态，但仍然有很多场景是这些智能服务所无法
覆盖的。比如很多物联网设备的智能计算，自动驾驶汽车，可穿戴设备等等，在这些具有相当实时性
要求的边缘智能设备上，远在云端的请求延时是无法容忍的。除此之外，很多边缘设备也无法接入
到云服务中，在大多情形下，这些设备需要独立运行，自我决策。而即便是可以计入云计算的设备，
智能物联网设备生成的庞大的数据量，也让云方案的可规模化大打折扣，同时大规模数据的传输也
让网络陷入极大的压力。因此，在边缘设备上仍然需要执行相当一部分的计算负载，以实现安全，
自动，实时的智能决策。而这又同边缘嵌入式设备有限的计算资源和极低的功耗相矛盾，特别是智能
应用往往是计算密度极高的，这为实现边缘智能（edge AI）带来了相当的挑战。

\subsection{ARM 架构计算设备的应用前景}
基于ARM 架构的芯片的计算应用不仅仅是在嵌入式计算场景下的主导，同时近年来，基于Arm的处理器
已经出现在HPC领域，为主要由x86处理器主导现状提供了一种替代方案。对于机器学习应用而言，
各类服务器级别的处理器，比如x86架构的处理器（Intel，AMD）和GPGPU（Nvidia），目前均已经有
了相对成熟的生态和广泛的应用。ARM架构的处理器为这一现状提供了一种额外的可能和新的选择。
Marvell（ThunderX2），Ampere（eMAG），华为（Kunpeng 920），富士通（A64FX）和
亚马逊（Graviton）等处理器制造商均推出基于Arm的服务器级处理器，将HPC 应用向成本更低，计算
性价比更高的ARM平台。

ThunderX2处理器时Marvell的第一个大规模商用的服务器级处理器。 ThunderX2处理器使用Armv8指
令集，并且专门针对服务器工作负载而设计。 Arm Neon技术是Arm Cortex-A和Cortex-R系列处理器的高级单指令多数据（SIMD）架构扩展。
Neon寄存器被视为相同数据类型元素的向量，Neon指令同时对多个元素进行操作。 NEON支持多种数据类型，
包括浮点和整数运算。NEON旨在通过加速音频和视频的编码和解码，用户界面，2D / 3D图形和游戏来
改善多媒体体验。 Neon还可以加速信号处理算法和功能，以加快诸如音频和视频处理，语音和面部识别，
计算机视觉和深度学习之类的应用程序。

\subsection{卷积网络在ARM 设备上的应用现状与对应计算优化}

卷积神经网络的使用使得嵌入式场景的视觉应用获得了普遍的效果提升。
而移动端的ARM设备同桌面端及服务端具备大量计算资源的设备不同，ARM移动设备往往具有低功耗，
计算能力相对较低，内存有限的劣势，而将计算密集的卷积网络计算部署在ARM设备也需要针对性的
实现多种优化。

模型量化方法是一种针对于模型运算加速和功耗的降低均有着显著效益的网络模型轻量
化方法，这一领域的研究证明，深度神经网络在低精度的表示下仍然具有着相当的准确率和在对应任务
上的表现力。 具有量化参数的低位数学运算与对神经网络的中间计算进行量化相结合，可带来较大的
计算增益和更高的性能。除了性能优势之外，由于内存访问成本降低同时计算效率提升，
量化神经网络还提高了功耗效率。 使用低位量化数据仅需要较少的片内和片外数据移动，从而减少了存
储器带宽并节省了大量能量。 较低精度的数学运算（例如8位整数乘法）消耗更少的能
量并提高了计算效率，从而降低了功耗。 此外，减少用于表示神经网络参数的位数可以减少存储空间。
而在产业界的应用中，结合嵌入式设备硬件条件，实现对于量化网络（8位整数矩阵）计算的原生支持以
及真正意义上的端到端low precision 计算，是实现网络模型性能加速，功耗降低的重要途经。

对于视觉任务而言，卷积神经网络的应用占据着主导地位。在这一网络模型中，卷积操作占据了绝大
多数的计算，从而性能加速和功耗优化的瓶颈为卷积操作的实现。而在卷积的计算实现中，目前主要
有GEMM-based Convolution (im2col) 和快速卷积算法
（FFT-based Convolution， Winograd Convolution）。基于GEMM的卷积算法的实现相对简单，
在对数据实现简单的重组之后，便可以直接调用成熟的矩阵乘法实现卷积操作。而快速卷积方法，
尽管具有比较高的实现难度，但可以有效实现卷积计算复杂度的降低。
结合应用于移动端的轻量化网络的特点，网络中主要的
卷积操作，其卷积核均为3x3。在理论研究和实践中，winograd 快速卷积方法对于3x3卷积具有明显
的计算复杂度优势。
而另外一方面，结合量化网络参数的特点，由于winograd卷积自身的实现复杂性以及量化计算中数值表示精度的频繁变换，
使得适用于于桌面端以及浮点运算的Winograd算法不能高效实现。目前在移动端最为通用的卷积算法为im2col 及其变种。
而Winograd 卷积的量化计算实现，目前在研究与应用中都不够成熟。

此外，卷积网络中的计算瓶颈在于卷积计算，而卷积计算的实现往往同矩阵乘法密切相关。主流的卷积
实现方法都会直接或间接的将卷积运算转换为矩阵乘法计算。 因此，卷积神经网络
中的有效推理问题中的一大重点和难点在于矩阵乘法（在线性代数库中也称为GEMM）的有效实现。
尽管HPC 领域已经对于GEMM有了相当成熟的研究成果，但HPC领域对于GEMM的研究则是针对服务端具有
大量的计算资源和充分的计算能力的x86架构的设备，同在边缘计算领域资源和能力均受限，并且以ARM
架构设备为主导的场景存在着较大的偏差，而在这种场景下的量化计算则更是寥寥无几。另外，高性能
计算中的矩阵乘法所关注的问题往往是规模较大的矩阵，而部署在移动端的神经网络中所涉及的矩阵运算
往往没有达到这种规模，因此很多针对大矩阵运算的优化策略，在这里是多余的，同时还会带来额外的
开销。

结合现有的针对于移动端网络模型以及模型轻量化
方法研究，充分利用硬件平台的计算能力，结合快速卷积方法，对于移动端应用的算子库（kernel library）
实现优化设计，对于实现高效，低功耗的深度网络模型的部署，
边缘智能，或者说智能物联网（AIoT）具备着不言而喻的重要性和
必要性。


\section{本文主要研究内容与贡献}

\subsection{主要工作及贡献}

本文针对于实现ARM 设备上的快速卷积算法进行研究，实现在实际硬件上对于卷积操作以及卷积网络通过
量化计算和快速卷积算法的效率加速。主要内容和贡献如下：

\begin{itemize}

  \item 针对Winograd卷积方法的整数卷积优化问题，提出了局部区域多通道Winograd卷积算法，针对于嵌入式设备有限的计算资源，充分利用了卷积操作的空间局部性和Winograd的算法的通道可并行化，有效解决了整数卷积操作在嵌入式设备的加速问题。

  \item 为了进一步优化整数卷积的速度，利用ARM的NEON指令的高效并行计算能力，针对于移动端轻量级卷积网络的参数特征，提出了中小规模整数矩阵乘法缓存友好的高吞吐实现策略，从而更好地利用有限的存储和计算并行能力，避免传统高性能计算中的矩阵乘法的额外开销，进一步加速了整数卷积在ARM Cortex-A架构上的速度。

  \item 将优化的卷积算法集成到QNNPACK，并与TensorFlow Lite的量化卷积速度进行了对比，验证了所提方法的有效性。

\end{itemize}

\subsection{论文组织结构}

本篇论文的组织结构如下：

第一章介绍了当前边缘设备实现深度网络模型的运行的应用和现状，分析了量化神经网络在移动端应用的
可行性，以及快速卷积算法量化计算在移动端的应用的空缺和前景。并简要阐述了本文的主要工作和结构 。

第二章 系统总结了当前在卷积操作计算加速方法，神经网络量化研究以及网络量化计算在当前实现工作中的
支持。对于卷积操作的直接实现，基于GEMM的实现和快速卷积方法的实现展开说明，并分析各个方法的优劣；
通过总结网络模型量化研究，论证神经网络模型具有在低精度数值表示下仍然在对应任务上保持高准确率的
能力，并且存在可行的方法获得模型的量化表示；最后，结合业界对于量化卷积的实现，说明量化卷积计算
特别是量化快速卷积在ARM设备实现的应用前景，并分析当前实现所存在的缺陷。

第三章 主要阐释了Winograd 卷积在ARM 设备实现中的一种对于当前主流ARM硬件较为友好的方法。同时
对于HPC 领域提出的高效GEMM 算法针对性的在ARM 边缘计算场景和现代卷积网络量化计算的特征和限制
做了针对性的改进和调优，为Winograd 卷积在嵌入式量化计算的实现奠定基础。

第四章主要总结并分析了8位无符号整数的 Winograd 卷积计算在 ARMv8-A 架构下的实现。总结了实现
过程中需要考虑的硬件因素，快速卷积方法在实现中的额外优化，分析了这一卷积实现的有效性，以及
集成于QNNPACK实现卷积网络加速的效果。

第五章总结了本篇论文的主要研究内容和贡献点，并对当前方法的缺陷进行进一步分析，提出了未来研究工作的相关思路和展望。
