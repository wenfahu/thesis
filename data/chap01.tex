% !TeX root = ../main.tex

\chapter{带 English 的标题}
\label{cha:intro}

神经网络赋能嵌入式设备智能是当前智能应用的一大热点。作为神经网络与边缘计算的交叉领域，
这一研究需要调和深度神经网络模型计算资源消耗巨大与嵌入式设备端计算资源有限，功耗低的矛盾，
权衡模型的精度和在嵌入式硬件环境中的运行效率，实现智能应用的高效部署。一方面，在网络模型
设计中，很多适用于低计算复杂度的轻量级模型相继提出，如Mobile Net系列，MnasNet 等，实现
了在低参数量和低计算复杂度的场景在对应任务场景下的高精度表现。另外一方面，关于神经网络的
压缩方法的研究如各类剪枝、量化方法也取得了较为成熟的应用，使得深度网络模型的计算得以简化。


智能手机作为使用最为广泛和频繁的嵌入式设备也便成为边缘智能最为热门的应用场景。同时，智能
化的服务也会为智能手机的使用带来颠覆性的体验。目前主流的智能手机的硬件环境均使用ARMv8指
令架构的芯片，而这一系列的芯片具有SIMD扩展NEON，这一功能使得在ARM平台上实现高效的矩阵计
算成为可能。而矩阵计算正是神经网络模型在计算中的核心。“GEMM is at the heart of deep
 learning”（Pete Warden，technical lead of the mobile and embedded TensorFlow 
 group, in Google's Brain team）。

尽管云计算的高速发展和越来越快的网络技术（5G网络）正在深刻的改变着数据获取以及被处理的
手段，很多计算和业务也有着向云端靠拢的趋势，但是在可见的未来，在边缘设备上智能计算的需求
明显是远远高于在云端的。尽管云端的智能计算比如Microsoft Azure Conginitive Serivce，
AWS SageMaker等服务具有丰富的计算资源和应用生态，但仍然有很多场景是这些智能服务所无法
覆盖的。比如很多物联网设备的智能计算，自动驾驶汽车，可穿戴设备等等，在这些具有相当实时性
要求的边缘智能设备上，远在云端的请求延时是无法容忍的。除此之外，很多边缘设备也无法接入
到云服务中，在大多情形下，这些设备需要独立运行，自我决策。而即便是可以计入云计算的设备，
智能物联网设备生成的庞大的数据量，也让云方案的可规模化大打折扣，同时大规模数据的传输也
让网络陷入极大的压力。因此，在边缘设备上仍然需要执行相当一部分的计算负载，以实现安全，
自动，实时的智能决策。而这又同边缘嵌入式设备有限的计算资源和极低的功耗相矛盾，特别是智能
应用往往是计算密度极高的，这为实现边缘智能（edge AI）带来了相当的挑战。

在图像/视频处理领域，卷积神经网络的助力使得嵌入式场景的应用获得了普遍的效果提升。并且在
卷积网络计算的优化层面，也有将Winograd 方法，Cook-Toom方法，Strasson矩阵乘法等方法用
于优化卷积计算。在移动设备和嵌入式计算场景下，ARM Cortex-A 系列芯片是使用最为广泛的基于
ARM  架构的SoC。高性能程序的设计同相关的硬件平台密切相关，而针对于Cortex-A 架构，快速
卷积算法（Winograd 卷积）的实现也不例外。

为实现在高效，低功耗的深度网络模型的部署，结合现有的移动端针对性的网络模型以及模型轻量化
方法研究，以及充分利用硬件平台的计算能力，需要对于在移动端应用的算子库（kernel library）
实现优化设计。模型量化方法是一种针对于模型运算加速和功耗的降低均有着显著效益的网络模型轻量
化方法，这一领域的研究证明，深度神经网络在低精度的表示下仍然具有着相当的准确率和在对应任务
上的表现力。 具有量化参数的低位数学运算与对神经网络的中间计算进行量化相结合，可带来较大的
计算增益和更高的性能。除了性能优势之外，量化神经网络还提高了功耗效率，其原因有两个：降低的
内存访问成本和提高的计算效率。 使用低位量化数据需要更少的片内和片外数据移动，从而减少了存
储器带宽并节省了大量能量。 较低精度的数学运算（例如8位整数乘法与32位浮点乘法）消耗更少的能
量并提高了计算效率，从而降低了功耗。 此外，减少用于表示神经网络参数的位数可以减少存储空间。
而在产业界的应用中，结合嵌入式设备硬件条件实现对于量化网络（8位整数矩阵）计算的原生支持以
及真正意义上的端到端low precision 计算对于实现网络模型性能加速，功耗降低的重要途经。

对于视觉任务而言，卷积神经网络的应用占据着主导地位。在这一网络模型中，卷积操作占据了绝大
多数的计算，从而性能加速和功耗优化的瓶颈为卷积操作的实现。而在卷积的计算实现中，目前主要
有GEMM-based Convolution (im2col, indirection Convolution) 和快速卷积算法
（FFT-based Convolution， Winograd Convolution）。结合移动端网络的特点，网络中主要的
卷积操作，其卷积核均为3x3。在理论研究和实践中，winograd 快速卷积方法对于3x3卷积具有明显
的效率优势。而另外一方面，结合量化网络的实现，由于winograd卷积的实现复杂性以及winograd 
transform中存在的数值表示的限制，目前最为通用的在移动端应用的卷积算法为im2col 及其memory 
优化改进版本 indirection convolution。我们的kernel library中通过复数域的winograd 变换
插值，实现使用8 bit整数表示的winograd 卷积，尝试在移动端的卷积计算中获得更高的效率和高准
确率。

用于计算机视觉的神经网络将其大部分推理时间花费在卷积和完全连接层（Fully Connected）的运算上。
这些运算符与矩阵矩阵乘法紧密相关：完全连接运算和1×1卷积直接映射到矩阵矩阵乘法，具有较大内核
的卷积可以通过im2col一类的内存布局变换方法将卷积问题转换为矩阵乘法问题。 因此，卷积神经网络
中的有效推理问题主要是矩阵-矩阵乘法的有效实现问题（在线性代数库中也称为GEMM）。

与单精度甚至半精度浮点相比，低精度整数表示具有多个优点：内存占用空间小2到4倍，这有助于将神经
网络模型保留在移动处理器的小型缓存中； 改进了内存带宽绑定操作的性能； 提高能源效率； 在许多
类型的硬件上，计算吞吐量更高。


另外一方面，针对于low precision 的参数表示和移动端arm 处理器的硬件特征（cpu cache），
需要对于传统的应用于高性能计算的GEMM 方法实现优化。传统的GEMM实现中往往对矩阵重新打包以
更好地利用缓存层次结构，以期在大量计算上分摊打包开销，而在low precision的网络表示中矩阵
中经过分治的sub panel往往是可以放入CPU的L1缓存的，对于这种情况的针对性优化可以删除所有
不是计算必需的内存转换。同时由于输入的sub panel可以完全放入CPU L1 Cache，这样也可以避免
在每次矩阵计算的micro kernel call中对于输入的重新打包，而在传统的GEMM实现中，这一操作却
不得不是必须的，由于limited cache associativity，在每次micro kernel中不得不读取可能用
于计算的输入进入cache，在此过程中，sub panel中的不同行的值可能落入同一cache set，造出缓
存冲突，从而导致性能下降。而这一情形，在量化参数的表示下将不再成为问题。


基于ARM 架构的芯片的计算应用不仅仅是在嵌入式计算场景下的主导，同时近年来，基于Arm的处理器
已经出现在HPC领域，为主要由x86处理器主导现状提供了一种替代方案。对于机器学习应用而言，
各类服务器级别的处理器，比如x86架构的处理器（Intel，AMD）和GPGPU（Nvidia），目前均已经有
了相对成熟的生态和广泛的应用。ARM架构的处理器为这一现状提供了一种额外的可能和新的选择。
Marvell（ThunderX2），Ampere（eMAG），华为（Kunpeng 920），富士通（A64FX）和
亚马逊（Graviton）等处理器制造商均推出基于Arm的服务器级处理器，将HPC 应用向成本更低，计算
性价比更高的ARM平台。

ThunderX2处理器时Marvell的第一个大规模商用的服务器级处理器。 ThunderX2处理器使用Armv8指
令集，并且专门针对服务器工作负载而设计。 该设计包括八个DDR4存储器通道，每个双插槽节点可提
供超过220 GB / s的实测STREAM三合一存储器带宽。

Arm Neon技术是Arm Cortex-A和Cortex-R系列处理器的高级单指令多数据（SIMD）架构扩展。

Neon寄存器被视为相同数据类型元素的向量，Neon指令同时对多个元素进行操作。 NEON支持多种数据类型，
包括浮点和整数运算。NEON旨在通过加速音频和视频的编码和解码，用户界面，2D / 3D图形和游戏来
改善多媒体体验。 Neon还可以加速信号处理算法和功能，以加快诸如音频和视频处理，语音和面部识别，
计算机视觉和深度学习之类的应用程序。
