% !TeX root = ../main.tex

\chapter{引言}
\label{cha:intro}

\section{研究背景}

神经网络赋能嵌入式设备智能是当前智能应用的一大热点。作为神经网络与边缘计算的交叉领域，
这一研究需要调和深度神经网络模型计算资源消耗巨大与嵌入式设备端计算资源有限，功耗低的矛盾，
权衡模型的精度和在嵌入式硬件环境中的运行效率，实现智能应用的高效部署。一方面，在网络模型
设计中，很多适用于低计算复杂度的轻量级模型相继提出，如Mobile Net系列，MnasNet 等，实现
了在低参数量和低计算复杂度的场景在对应任务场景下的高精度表现。另外一方面，关于神经网络的
压缩方法的研究如各类剪枝、量化方法也取得了较为成熟的应用，使得深度网络模型的计算得以简化。


智能手机作为使用最为广泛和频繁的嵌入式设备也便成为边缘智能最为热门的应用场景。同时，智能
化的服务也会为智能手机的使用带来颠覆性的体验。目前主流的智能手机的硬件环境均使用ARMv8指
令架构的芯片，而这一系列的芯片具有SIMD扩展NEON，这一功能使得在ARM平台上实现高效的矩阵计
算成为可能。而矩阵计算正是神经网络模型在计算中的核心。“GEMM is at the heart of deep
 learning”（Pete Warden，technical lead of the mobile and embedded TensorFlow 
 group, in Google's Brain team）。

\subsection{边缘智能计算的必要性}

尽管云计算的高速发展和越来越快的网络技术（5G网络）正在深刻的改变着数据获取以及被处理的
手段，很多计算和业务也有着向云端靠拢的趋势，但是在可见的未来，在边缘设备上智能计算的需求
明显是远远高于在云端的。尽管云端的智能计算比如Microsoft Azure Conginitive Serivce，
AWS SageMaker等服务具有丰富的计算资源和应用生态，但仍然有很多场景是这些智能服务所无法
覆盖的。比如很多物联网设备的智能计算，自动驾驶汽车，可穿戴设备等等，在这些具有相当实时性
要求的边缘智能设备上，远在云端的请求延时是无法容忍的。除此之外，很多边缘设备也无法接入
到云服务中，在大多情形下，这些设备需要独立运行，自我决策。而即便是可以计入云计算的设备，
智能物联网设备生成的庞大的数据量，也让云方案的可规模化大打折扣，同时大规模数据的传输也
让网络陷入极大的压力。因此，在边缘设备上仍然需要执行相当一部分的计算负载，以实现安全，
自动，实时的智能决策。而这又同边缘嵌入式设备有限的计算资源和极低的功耗相矛盾，特别是智能
应用往往是计算密度极高的，这为实现边缘智能（edge AI）带来了相当的挑战。

\subsection{ARM 架构计算设备的应用前景}
基于ARM 架构的芯片的计算应用不仅仅是在嵌入式计算场景下的主导，同时近年来，基于Arm的处理器
已经出现在HPC领域，为主要由x86处理器主导现状提供了一种替代方案。对于机器学习应用而言，
各类服务器级别的处理器，比如x86架构的处理器（Intel，AMD）和GPGPU（Nvidia），目前均已经有
了相对成熟的生态和广泛的应用。ARM架构的处理器为这一现状提供了一种额外的可能和新的选择。
Marvell（ThunderX2），Ampere（eMAG），华为（Kunpeng 920），富士通（A64FX）和
亚马逊（Graviton）等处理器制造商均推出基于Arm的服务器级处理器，将HPC 应用向成本更低，计算
性价比更高的ARM平台。

ThunderX2处理器时Marvell的第一个大规模商用的服务器级处理器。 ThunderX2处理器使用Armv8指
令集，并且专门针对服务器工作负载而设计。 Arm Neon技术是Arm Cortex-A和Cortex-R系列处理器的高级单指令多数据（SIMD）架构扩展。
Neon寄存器被视为相同数据类型元素的向量，Neon指令同时对多个元素进行操作。 NEON支持多种数据类型，
包括浮点和整数运算。NEON旨在通过加速音频和视频的编码和解码，用户界面，2D / 3D图形和游戏来
改善多媒体体验。 Neon还可以加速信号处理算法和功能，以加快诸如音频和视频处理，语音和面部识别，
计算机视觉和深度学习之类的应用程序。

\subsection{卷积网络在ARM 设备上的应用现状与对应计算优化}

在图像/视频处理领域，卷积神经网络的助力使得嵌入式场景的应用获得了普遍的效果提升。并且在
卷积网络计算的优化层面，也有将Winograd 方法，Cook-Toom方法，Strasson矩阵乘法等方法用
于优化卷积计算。在移动设备和嵌入式计算场景下，ARM Cortex-A 系列芯片是使用最为广泛的基于
ARM  架构的SoC。高性能程序的设计同相关的硬件平台密切相关，而针对于Cortex-A 架构，快速
卷积算法（Winograd 卷积）的实现也不例外。

为实现在高效，低功耗的深度网络模型的部署，结合现有的移动端针对性的网络模型以及模型轻量化
方法研究，以及充分利用硬件平台的计算能力，需要对于在移动端应用的算子库（kernel library）
实现优化设计。模型量化方法是一种针对于模型运算加速和功耗的降低均有着显著效益的网络模型轻量
化方法，这一领域的研究证明，深度神经网络在低精度的表示下仍然具有着相当的准确率和在对应任务
上的表现力。 具有量化参数的低位数学运算与对神经网络的中间计算进行量化相结合，可带来较大的
计算增益和更高的性能。除了性能优势之外，量化神经网络还提高了功耗效率，其原因有两个：降低的
内存访问成本和提高的计算效率。 使用低位量化数据需要更少的片内和片外数据移动，从而减少了存
储器带宽并节省了大量能量。 较低精度的数学运算（例如8位整数乘法与32位浮点乘法）消耗更少的能
量并提高了计算效率，从而降低了功耗。 此外，减少用于表示神经网络参数的位数可以减少存储空间。
而在产业界的应用中，结合嵌入式设备硬件条件实现对于量化网络（8位整数矩阵）计算的原生支持以
及真正意义上的端到端low precision 计算对于实现网络模型性能加速，功耗降低的重要途经。

对于视觉任务而言，卷积神经网络的应用占据着主导地位。在这一网络模型中，卷积操作占据了绝大
多数的计算，从而性能加速和功耗优化的瓶颈为卷积操作的实现。而在卷积的计算实现中，目前主要
有GEMM-based Convolution (im2col, indirection Convolution) 和快速卷积算法
（FFT-based Convolution， Winograd Convolution）。结合移动端网络的特点，网络中主要的
卷积操作，其卷积核均为3x3。在理论研究和实践中，winograd 快速卷积方法对于3x3卷积具有明显
的效率优势。而另外一方面，结合量化网络的实现，由于winograd卷积的实现复杂性以及winograd 
transform中存在的数值表示的限制，目前最为通用的在移动端应用的卷积算法为im2col 及其变种。
而Winograd 卷积 这一在卷积计算的复杂性而言当前最优的一种卷积算法的量化计算实现，目前在
研究与应用中都是相对空白的。而实现使用8 bit整数表示的winograd 卷积，将在移动端的卷积计
算中获得更高的效率。

用于计算机视觉的神经网络将其大部分推理时间花费在卷积和完全连接层（Fully Connected）的运算上。
这些运算符与矩阵矩阵乘法紧密相关：完全连接运算和1×1卷积直接映射到矩阵矩阵乘法，具有较大内核
的卷积可以通过im2col一类的内存布局变换方法将卷积问题转换为矩阵乘法问题。 因此，卷积神经网络
中的有效推理问题主要是矩阵-矩阵乘法的有效实现问题（在线性代数库中也称为GEMM）。

与单精度甚至半精度浮点相比，低精度整数表示具有多个优点：内存占用空间小2到4倍，这有助于将神经
网络模型保留在移动处理器的小型缓存中； 改进了内存带宽绑定操作的性能； 提高能源效率； 在许多
类型的硬件上，计算吞吐量更高。

另外一方面，针对于low precision 的参数表示和移动端arm 处理器的硬件特征（cpu cache），
需要对于传统的应用于高性能计算的GEMM 方法实现优化。传统的GEMM实现中往往对矩阵重新打包以
更好地利用缓存层次结构，以期在大量计算上分摊打包开销，而在low precision的网络表示中矩阵
中经过分治的sub panel往往是可以放入CPU的L1缓存的，对于这种情况的针对性优化可以删除所有
不是计算必需的内存转换。同时由于输入的sub panel可以完全放入CPU L1 Cache，这样也可以避免
在每次矩阵计算的micro kernel call中对于输入的重新打包，而在传统的GEMM实现中，这一操作却
不得不是必须的，由于limited cache associativity，在每次micro kernel中不得不读取可能用
于计算的输入进入cache，在此过程中，sub panel中的不同行的值可能落入同一cache set，造出缓
存冲突，从而导致性能下降。而这一情形，在量化参数的表示下将不再成为问题。

尽管HPC 领域已经对于GEMM有了相当成熟的研究成果，但HPC领域对于GEMM的研究则是针对服务端具有
大量的计算资源和充分的计算能力的x86架构的设备，同在边缘计算领域资源和能力均受限，并且以ARM
架构设备为主导的场景存在着较大的偏差，而在这种场景下的量化计算则更是寥寥无几。所以针对性的
在边缘设备中实现优化的量化计算对于边缘智能，或者说智能物联网（AIoT）具备着不言而喻的重要性和
必要性。

\section{本文主要研究内容与贡献}

\subsection{主要工作及贡献}

本文针对于实现ARM 设备上的快速卷积算法进行研究，实现在实际硬件上对于卷积操作以及卷积网络通过
量化计算和快速卷积算法的效率加速。主要内容和贡献如下：

\begin{itemize}
    \item 研究了在ARM设备上针对性的Winograd卷积算法优化;
          从Winograd 卷积实现的各个步骤，结合ARM设备上硬件特性，比如Cache 机制，SIMD 实现，
          可用于计算的寄存器的数目，CPU Pipeline机制等，对于Winograd算法针对性的重做设计，
          以最大化利用ARM设备的计算能力。
    \item 实现了在量化计算支持下的矩阵乘法和Winograd卷积算法;
          在嵌入式硬件环境下，实现满足移动设备网络模型量化计算场景需求的高效矩阵乘法，并以此
          为基础实现高效的量化快速卷积操作，实现在移动端的卷积计算加速和卷积网络效率提升。
\end{itemize}

\subsection{论文组织结构}

本篇论文的组织结构如下：

第一章介绍了当前边缘设备实现深度网络模型的运行的应用和现状，分析了量化神经网络在移动端应用的
可行性，以及快速卷积算法量化计算在移动端的应用的空缺和前景。并简要阐述了本文的主要工作和结构 。

第二章 系统总结了当前在卷积操作计算加速方法，神经网络量化研究以及网络量化计算在当前实现工作中的
支持。对于卷积操作的直接实现，基于GEMM的实现和快速卷积方法的实现展开说明，并分析各个方法的优劣；
通过总结网络模型量化研究，论证神经网络模型具有在低精度数值表示下仍然在对应任务上保持高准确率的
能力，并且存在可行的方法获得模型的量化表示；最后，结合业界对于量化卷积的实现，说明量化卷积计算
特别是量化快速卷积在ARM设备实现的应用前景，并分析当前实现所存在的缺陷。

第三章 主要阐释了Winograd 卷积在ARM 设备实现中的一种对于当前主流ARM硬件较为友好的方法。同时
对于HPC 领域提出的高效GEMM 算法针对性的在ARM 边缘计算场景和现代卷积网络量化计算的特征和限制
做了针对性的改进和调优，为Winograd 卷积在嵌入式量化计算的实现奠定基础。

第四章主要总结并分析了8位无符号整数的 Winograd 卷积计算在 ARMv8-A 架构下的实现。总结了实现
过程中需要考虑的硬件因素，快速卷积方法在实现中的额外优化，分析了这一卷积实现的有效性，针对于
全精度计算的加速效果和内存占用优化，相对于同类型量化卷积计算的效率优势，以及结合Winograd卷积
的数值稳定性特征分析这一实现在对应视觉任务下的准确性。

第五章总结了本篇论文的主要研究内容和贡献点，并对当前方法的缺陷进行进一步分析，提出了未来研究工作的相关思路和展望。